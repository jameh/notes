***********************************************************
Joint Source Coding Theorem & Continuous Information Theory
***********************************************************
Diagram:

Source :math:`\mathcal V\to^{V^n}` Source Enc. :math:`f(\cdot)\to^{X^n}` channel :math:`p_{Y^n|X^n}\to^{Y^n}` SC decoder :math:`g(\cdot)\to \hat V` message word estimate

.. admonition:: Definition

    Consider a source :math:`\{V_i\}_{i=1}^\infty` with finite alphabet :math:`\mathcal V` and discrete channel :math:`(\mathcal X, \mathcal Y, \{p_{X^n|Y^n}\}_{n=1}^\infty)` A (joint) **source-channel** code of rate 1 and blocklength :math:`n` is a pair of maps :math:`(f,g)`

    .. math::
        f: \mathcal V^n\to\mathcal X^n\text{ encoder}\\
        g: \mathcal Y^n\to\mathcal V^n\text{ decoder}

    To send source word :math:`V^n=(v_1,...,v_n)`, encoder transmits :math:`X^n(V^n)=f(V^n)` over the channel. At receiver, :math:`Y^n` is received via the channel law :math:`P_{Y^n|X^n}` and estimate transmitted message word as :math:`\hat V^n=g(Y^n)`.

    An **error** is made: :math:`V^n\neq \hat V^n`

    Probability of decoding error:

    .. math::
        P_e^{(n)}&=P[V^n\neq \hat V^n]\\
                 &=\sum_{v^n\in\mathcal V^n}\sum_{y^n\in\mathcal Y^n: g(y^n)\neq v^n}p_{V^n}(v^n)p_{Y^n|X^n}(y^n|x^n(v^n))

.. admonition:: Lossless Joint Source-Channel Coding Theorem

    (Shannon's Lossless Information Transmission Theorem)

    (Shannon's Separation Principle)

    (i) Let :math:`\{V_n\}_{n=1}^\infty` be a source with finite alphabet :math:`\mathcal V` and entropy rate :math:`H(\mathcal V)` such that :math:`\{V_n\}_{n=1}^\infty` satisfies the **AEP**.

        Consider a DMC :math:`(\mathcal X,\mathcal Y, p_{Y|X}` with capacity :math:`C=max_{p_X} I(X;Y)`.

        Then if :math:`H(\mathcal V)<C`, then there exist a sequence of rate-1 source-channel codes of blocklength :math:`n` s.t.

        .. math::
            \lim_{n\to\infty}p_e^{(n)}=0


        Proof:

        Direct application of Lossless Source-Coding Theorem followed by Lossless Channel-Coding Theorem

    (ii) Conversely, for any **stationary** source :math:`\{V_n\}`, if
          
        .. math::
            H(\mathcal V)>C

        then the probability of error :math:`p_e^{(n)}` of any rate-1 source-channel coding system is **bounded away from zero** and it is *not possible* to reliably send the source over the channel.

More generally
==============
Source :math:`\mathcal V\to^{V^k}` Source Enc. :math:`f(\cdot)\to^{X^n}` channel :math:`p_{Y^n|X^n}\to^{Y^n}` SC decoder :math:`g(\cdot)\to \hat V^k` message word estimate

.. math::
    \frac{k}{n}\text{ source symbols / channel symbol}

Then the Theorem becomes:

(i) If :math:`\frac{k}{n}H(\mathcal V)<C` then *reliable* communication of the source over the channel via a rate :math:`\frac{k}{n}` SC code of coding blocklength :math:`n` is possible.
(ii) If :math:`\frac{k}{n}H(\mathcal V)>C`, then reliable comm. is not possible.

.. admonition:: Special Case

    If the source outputs 1 source symbol every :math:`τ_s` seconds, and the channel transmits 1 channel symbol every :math:`τ_c` seconds, then the Theorem becomes:

    system's rate:

    .. math::
        \frac{τ_c}{τ_s}

    (i) If :math:` \frac{τ_c}{τ_s}H(\mathcal V)<C` then *reliable* communication of the source over the channel via a rate :math:`\frac{k}{n}` SC code of coding blocklength :math:`n` is possible.
    (ii) If :math:` \frac{τ_c}{τ_s}H(\mathcal V)>C`, then reliable comm. is not possible.

.. admonition:: Remark

    When :math:`H(\mathcal V)=C`, this is still an open problem in general.

Proof of the Converse
=====================
(rate-1 system)

Need to show that :math:`\forall` sequences of SC codes of rate-1 and cod coding blocklength :math:`n` :math:`(f,g)`:

.. math::
    f:\mathcal V^n\to\mathcal X^n\\
    g:\mathcal Y^n\to\mathcal V^n

with :math:`P_e^{(n)}\to 0` as :math:`n\to\infty`, we must have that :math:`H(\mathcal V)\leq C`.

Fano's inequality:
------------------

.. math::
    \hat V=g(Y^n), P_e^{(n)}=P[V^n\neq \hat V^n],\\
    H(V|Y^n)&\leq h_b(P_e^{(n)})+P_e^{(n)}\log(|\mathcal V|^n-1)\\
            &\leq 1+ P_e^{(n)}\log|\mathcal V|^n\\
            &=1+P_e^{(n)}n\log|\mathcal V|

Now,

.. math::
    H(\mathcal V)&\leq \frac{1}{n}H(V^n)=\frac{1}{n}H(V^n|Y^n)+\frac{1}{n}I(V^n;Y^n)\\
    &\leq \frac{1}{n}+P_e^{(n)}\log|\mathcal V|+\frac{1}{n}I(V^n;Y^n)\\
    &\leq \frac{1}{n}+P_e^{(n)}\log|\mathcal V|+\frac{1}{n}I(X^n;Y^n)\\
    &\leq \frac{1}{n}+P_e^{(n)}\log|\mathcal V|+C

where the inequalities follow respecitvely from (*), Data processing theorem, :math:`nC\geq I(X^n;Y^n)`

Taking the limit :math:`n\to\infty`, since :math:`P_e^{(n)}\to 0` by assumption, we get that 

.. math::
    H(\mathcal V)=C

Information Theory for Continuous-Valued Systems
================================================
Differential Entropy
--------------------
Let :math:`X` be a real-valued ("continuous") random variable. :math:`X` is usually described by its cumulative distribution function (cdf):

.. math::
    F_X(x):=P[X\leq x]

for :math:`x\in\mathbb R`.

The distribution is "absolutely continuous" if a probability density function (pdf) :math:`f_X(\cdot)` exists:

.. math::
    F_X(x)=\int_{-\infty}^x f_X(t)dt

If :math:`F_X(\cdot)` is differentiable, then pdf :math:`f_X(x)=\frac{d}{dx}F_X(x)`.

We only deal with random variables that admit a **density**.