**********************************
Entropy Rate for Stationary Source
**********************************

Given a finite alphabet homogeneous Markov Chain :math:`\{X_i\}` with alphabet

.. math::
	\mathcal X = \{1,2,...,m\}

and state transition probability matrix

.. math::
	Q_{m\times m}=[p_{ij}]

where :math:`p_{ij}:=P[X_n=j|X_{n-1}=i]`, :math:`i,j \in \mathcal X`

then the probability distribution vector,

.. math::
	Π=(Π_1,Π_2,...,Π_m)

such that :math:`Π_l\geq 0`, :math:`l=1,2,...m` and :math:`\sum_{l=1}^m Π_l=1`

.. math::
	Π=Π Q

.. admonition:: Example

	Homogeneous Binary Markov Chain

	.. math::
		\mathcal X=\{0,1\}

		Q_{2\times 2}=[p_{ij}]=\left[\matrix{p_{00} & p_{01} \\ p_{10} & p_{11}}\right]

		= \left[\matrix{1-α & α \\ β & 1-β}\right]

	.. image::

	Solve for :math:`Π=(Π_0,Π_1)`:

	.. math::
		Π = Π Q

		(Π_0,1-Π_0)=(Π_0,1-Π_0)\left(\matrix{1-α & α \\ β & 1-β}\right)

		\implies Π_0=\frac{β}{α+β}

		Π_1=1-Π_0

	therefore,

	.. math::
		Π=(Π_0,Π_1)=(\frac{β}{α+β},\frac{α}{α+β})

	So if,

	.. math::
		P[X_1=0]=Π_0=1-P[X_1=1]

	then the MC is *stationary*.

Entropy Rate:
=============
For a DMS, we defined the source entropy as the entropy of any RV in the DMS. (since all its RVs are iid.)

Question:

How might we define the notion of entropy for a source with memory?

.. admonition:: Definition

	The *entropy rate* of a source :math:`\{X_i\}_{i=1}^\infty` with alphabet :math:`\mathcal X` is denoted by :math:`H(\mathcal X)` (or :math:`H_\infty(X)` or :math:`H(X_\infty)`) and defined by:

	.. math::
		H(\mathcal X):=\lim_{n\to \infty} \frac{1}{n}H(X_1,X_2,...X_n)

	provided that the limit exists.

.. note::
	For a DMS, we know that

	.. math::
		H(X_1,X_2,...,X_n)=H(X_1)+H(X_2)+...+H(X_n)

	by independence

	.. math::
		=nH(X_1)

	by identical distribution

	therefore,

	.. math::
		H(\mathcal X)=H(X_1)

.. admonition:: Lemma

	For a stationary source, :math:`\{X_i\}_{i=1}^\infty`, the conditional entropy, :math:`H(X_n|X_{n-1},...H_1)` is decreasing in :math:`n`, and has *a limit*:

	.. math::
		H'(\mathcal X):=\lim_{n\to \infty}H(X_n|X_{n-1},...X_1)

		H'(\mathcal X):=\lim_{n\to \infty}H(X_n|X^{n-1})

	Proof:

	.. math::
		H(X_n|X_{n-1},...,X_1)\leq H(X_n|X_{n-1},...,X_2)

	since conditioning reduces entropy

	.. math::
		=H(X_n,...X_2)-H(X_{n-1},...,X_2)

	by chain rule for entropy

	And, by stationarity,

	.. math::
		= H(X_{n-1},...,X_1)-H(X_{n-2},...,X_1)

		=H(X_{n-1}|X^{n-2})

	by the chain rule.

	Since :math:`H(C_n|X^{n-1})` is a decreasing sequence in :math:`n`, and is bounded below (by 0), it must have a limit:

	.. math::
		H'(\mathcal X)=\lim_{n\to\infty} H(X_n|X_{n-1})

.. admonition:: Theorem

	Cesaro-Mean Theorem

	If :math:`a_n\to_{n\to\infty} a`, and :math:`b_n=\frac{1}{n}\sum_{i=1}^\infty a_i`, then

	.. math::
		b_n\to_{n\to\infty} a

	Proof pg 76 in Textbook

.. admonition:: Theorem

	For a stationary source :math:`\{X_1\}_{i=1}^\infty` with alphabet :math:`\mathcal X`, its entropy rate :math:`H(\mathcal X)` always exists, and is given by:

	.. math::
		H(\mathcal X)=H'(\mathcal X)

		=\lim_{n\to \infty}H(X_n|X_{n-1},...,X_1)

	Proof:

	.. math::
		\frac{1}{n}H(X_1,...,X_n)=\frac{1}{n}\sum_{i=1}^n H(C_1|X^{i-1})

	by chain rule.

	But, we know that :math:`H(X_i|X^{i-1})` decreases with :math:`i`, and converges to :math:`H'(\mathcal X)` as :math:`i\to\infty`

	So by the Cesaro-Mean Theorem, 

	.. math::
		H(\mathcal X)=H'(\mathcal X)

	.. note::
		For any :math:`n`,

		.. math::
			H(\mathcal X)\leq H(X_n|X^{n-1})

		and

		.. math::
			\frac{1}{n}H(X_1,...,X_n)

		is decreasing in :math:`n`.

.. admonition:: Example

	Consider the stationary binary Markov Chain

	.. math::
		H(\mathcal X)=\lim_{n\to\infty}H(X_n|X^{n-1})

		= \lim_{n\to\infty}H(X_n|X_{n-1})

		=H(X_2|X_1)

		=-\sum_{i\in\mathcal X}\sum_{j\in\mathcal X}P[X_2=j|X_1=i]P[X_1=i]\log_2 P[C_2=j|Xi=i]

		=-\sum_{i\in\mathcal X}\sum_{j\in\mathcal X}p_{ij}Π_i\log_2 p_{ij}

		=\frac{β}{α+β}h_b(α)-\frac{α}{α+β}h_b(β)

	where :math:`h_b(c)=x\log_2 x - (1-x)\log_2 (1-x)` is the binary entropy function.

Stationary Ergodic Sources
==========================
Loosely speaking, stationary sources that cannot be separated into different persisting asymptotic modes of behaviour are known as *ergodic sources*.

A *stationary ergodic* source satisfies the Law of Large Numbers


