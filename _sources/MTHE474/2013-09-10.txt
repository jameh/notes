*******
Entropy
*******

.. admonition:: Theorem

	The **only** function defined over :math:`p \in [0,1]` satisfying the three desired properties from last lecture is:

	.. math::
		I(p) = -c \log_b p

	for a proof, see _`online lecture notes` Theorem 2.1

.. note::
	The choice of constant :math:`c` and base :math:`b` of the logarithm just determine units (recall :math:`\log_b u = \frac{\ln u}{\ln b}`)

So, we set :math:`c=1` and define:

.. table:: Units of information

	== =====
	b  units
	== =====
	2  bits
	e  nats
	== =====

unless specified, we assume that :math:`b=2`.

Hence, if you learn that:
  * a fair coin comes up heads:

	.. math::
		-\log_2(\frac{1}{2}) = 1 \text{ bit}

  * a fair die comes up 5, you've gained:

	.. math::
		-\log_2(\frac{1}{6}) = 2.585 \text{ bits of information}

For the first part of the course, we will deal with discrete time (discrete valued) systems

.. admonition:: Notation

	Let :math:`X` be a discrete random variable with finite alphabet :math:`\chi` and pmf :math:`p(x):=P[X=x]` for :math:`x \in \chi` or :math:`p_X(a):=P[X=a]` for :math:`a \in \chi`.

.. admonition:: Definition

	Entropy:

	The *entropy* of a discrete random variable :math:`X` with alphabet :math:`\chi` and pmf :math:`p_X(.)` is denoted by :math:`H(X)` or :math:`H(p_X)` or :math:`H(p)` and defined by:

	.. math::
		H(X) := -\sum_{x \in \chi} p(x) log_2 p(x)

.. note::

	:math:`H(X) = \sum_{x \in \chi} p(x) (-\log_2 p(x))`

	where the summand is the product of :math:`p(x)` and the self information of event :math:`X=x`.

	Thus, the entropy of a system is the *expected value* of self information for the system of events.

	.. math::
		H(X) = E_p[I(X)]

	where

	.. math::
		I(x) = -\log_2 p(x)

Therefore, :math:`H(X)` is the statistical average amount of information gained when you learn that one of the :math:`|\chi|` outcomes of :math:`X` has occured.

.. admonition:: Lemma

	(1) :math:`H(X)` is non-negative

		.. math::
			H(X) \geq 0

	(2)

		.. math::
			H_b(X):= -\sum_{x \in \chi} p(x) log_b p(x) = log_b 2 H(X)

		where :math:`H(X)` is in base 2.

.. admonition:: Example

	Binary Entropy function:

	.. math::
		\chi = \{0,1\}, 0 \leq p \leq 1

		P[X=0]=1-p

		P[X=1]=p

		H(X) = \sum_{x=0}^1 = p(x) log_2 p(x)

		= -p log_2 p -(1-p) log_2 (1-p) =: h(p)

.. image:: .static/Binary_entropy_plot.svg
	:width: 50%

The fundamental inequality (FI)
===============================
For the logarithm,

.. math::
	\ln x \leq x-1 \forall x \gt 0

with equality if and only if :math:`x=1`.

.. admonition:: Corollary

	.. math::
		\ln y \geq 1 - \frac{1}{y}

	with equality iff :math:`x=1`

	Proof: let :math:`y=\frac{1}{x}`

So, two useful forms of the Fundamental Inequality are:

.. math::
	\log_a x \leq \frac{1}{\ln a} (x-1)

	\log_a x \geq \frac{1}{\ln a} (1 - \frac{1}{x})

.. admonition:: Definition

	Joint Entropy

	Given two jointly distributed discrete random variables :math:`X` and :math:`Y`, (with alphabets :math:`\chi` and :math:`Y`) with join pmf

	.. math::
		p_{XY}(x,y) = p(x,y) = P(X=x, Y=y)

	where :math:`x \in \chi` and :math:`y \in Y`

	Their *joint entropy*, denoted :math:`H(X,Y)` is:

	.. math::
		H(X,Y) := E_{XY}[-log p(X,Y)]

		= -\sum_{x \in \chi} \sum_{y \in Y} p(x,y) \log_2 p(x,y)

.. admonition:: Definition

	Conditional Entropy

	The *conditional entropy* of random variable :math:`X` given :math:`Y` is

	.. math::
		H(X|Y) := E_{XY}[-log p(X|Y)]

		= - \sum_{x \in \chi} \sum_{y \in Y} p(x,y) \log_2 p(x|y)

	where

	.. math::
		p(x|y) = \frac{p(x,y)}{p(y)}


.. _`online lecture notes`: http://www.mast.queensu.ca/~math474/it-lecture-notes.pdf